{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implement a Training Loop for a Binary Classifier Network** üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "### **1 - Overview**  ###\n",
    "\n",
    "Welcome to the World of Neural Networks! ü§ñ‚ú®\n",
    "\n",
    "You have just embarked on a journey that has the potential to transform the way you think about solving problems with data.\n",
    "\n",
    "In this lesson, you will be guided through the process of building, training, and evaluating your own neural network, specifically, for a binary classification problem. But that's just the start! You will also get hands-on experience with layers, activation functions, optimizers, and the key concepts that power machine learning. \n",
    "\n",
    "By the end of this lesson, you will have the knowledge but also the skills to start experimenting with your own data and build powerful and yet customized neural networks. It's worth also mentioning that an intermediate level of Python is required, along with a basic understanding of how **classes** work.\n",
    "\n",
    "So, get ready to experiment and dive in ‚Äî your journey into deep learning starts now! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important Note on Submission to the AutoGrader** ##\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "Each assignment is labeled as **üéØ Challenge**, so whenever you see this, it‚Äôs time to solve the task! Make sure to complete every challenge, as your submission will only be considered if all challenges are completed. Also, whenever you encounter the '### YOUR CODE STARTS HERE ####' pattern, it's time for you to provide your solution, until you see the ### YOUR CODE ENDS HERE #### pattern.\n",
    "\n",
    "In this lesson, we‚Äôll have **5 challenges** for you to tackle! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "### **2 - Packages** ###\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. \n",
    "- [numpy](https://numpy.org/doc/) is the fundamental package for scientific computing with Python\n",
    "- [pandas](https://pandas.pydata.org/docs/) is a common package to analyze data\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python\n",
    "- [Pytorch](https://pytorch.org/docs/stable/index.html) is our selected framework to build Neural Networks!\n",
    "- [scikit-learn](https://scikit-learn.org/stable/) is a machine learning library that also provides many other functionalities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tests.unit_tests import *\n",
    "\n",
    "from utils.helper_functions import plot_decision_boundary, accuracy_fn\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "### **3 - Creating Our Dataset** ###\n",
    "\n",
    "To begin our journey, the first step is to generate a **synthetic dataset** that will be used for this lesson and training our neural network ü§ñ. We will use the `make_circles` function from scikit-learn, which creates a set of data points and arranged in two circles üîµüî¥. You might be asking why do we need a circle data set for this lesson? Well, it turns out that this type of dataset is ideal for classification tasks because it features a **non-linear decision boundary**, which is a common challenge in machine learning.\n",
    "\n",
    "With **1000 samples** and adding a small amount of noise, we will be mimicking a real-world scenario where the data is not always perfectly clean. Here's the code to generate the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "# create circle dataset:\n",
    "X, y = make_circles(n_samples,\n",
    "                   noise = 0.03,\n",
    "                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's utilize Pandas üêº package to understand better the structure of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles = pd.DataFrame({\"X1\": X[:, 0], \"X2\": X[:, 1],\n",
    "                       \"label\": y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "#### **3.1 - Visualizing the Dataset** üîç\n",
    "\n",
    "Now it's time to visualize it! By plotting the data points, we can better understand how the data is distributed and how is the relationship between the features and the target labels. We will leverage the **plt.scatter** method from matplotlib to create our visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(x = X[:, 0],\n",
    "            y = X[:, 1],\n",
    "            c = y,\n",
    "            cmap = plt.cm.RdYlBu # Red-Yellow-Blue Colormap\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "#### **3.2 - Converting Data to Tensors and Creating Train-Test Splits** üîÑ\n",
    "\n",
    "To begin training our neural network, we need to convert our dataset into **PyTorch tensors**, which are essential for working with neural networks in PyTorch. Remember from previous lessons, that **Tensors** allow us an efficient computation, especially when working with huge datasets!\n",
    "\n",
    "In addition, we'll split our data into **training** and **testing**. Recall from earlier lessons that this is an essential step in any Machine Learning project: **we need to expose our trained model to unseen data, in order to verify that it has learned üß† the patterns of the data üìà properly**.  \n",
    "\n",
    "Let‚Äôs proceed with these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X), X.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. of train examples: ', len(X_train))\n",
    "print('No. of test examples: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. of train examples: ', len(y_train))\n",
    "print('No. of test examples: ', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data converted into tensors and the properly split, we are now ready to feed it into our neural network model and start training! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "### **4 - Building the Model** ‚öôÔ∏èüß†\n",
    "\n",
    "In this section, you will get get *hands-on* experience in building a neural network model for a binary classifier! üí™ \n",
    "\n",
    "But don‚Äôt worry üò∞, we have got you coveredüõ°Ô∏è! Even if the content feels overwhelming or you need to review certain parts of our previous lessons, we will provide all the necessary materials to help you understand every step.\n",
    "\n",
    "We will begin by setting up our device (CPU) **(1)** for the model to use during training. Then, we will move on to **building the model architecture (2)** by subclassing `nn.Module`, which is the core-building-block of all neural networks in PyTorch. After that, we will **define a loss function and optimizer (3)**, which are essential for training our model and improving its performance. Finally, we will proceed to **training and test loops (4)** to put everything together and start the learning process.\n",
    "\n",
    "Let‚Äôs get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.1'></a>\n",
    "#### **4.1 - Our First Neural Network Architecture** üß†\n",
    "\n",
    "Your goal is to build the necessary artifacts for our Binary Classifier Neural Network, which follows the next architecture (shown in the image below):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h2>Binary Classifier Neural Network </h2>\n",
    "    <img src=\"./utils/images/nn_1.png\" style=\"width:600px;height:300px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very basic diagram for you to interpret. As you can see, it consists on:\n",
    "\n",
    "1. **Input layer:** our input feature matrix [$X_1$, $X_2$]\n",
    "2. **Hidden layer:**  our layer of neurons (nodes) that lies between input layer and output layer. <span style=\"color: rgb(181, 206, 168);\">The number of neurons for our very first proposed architecture is 5 </span>. Remember the core concepts of the Hidden Layer:\n",
    "\n",
    "    - The Hidden layer(s) help to extract relevant features from the input layer. <span style=\"color: rgb(181, 206, 168);\">Our first architecture only consists of 1 layer</span> (tempted to add more? üòè) \n",
    "    - Each neuron in the hidden layer applies a mathematical transformation to the incoming data. Activation functions like ReLU, sigmoid, and tanh introduce non-linearity into the model, allowing it to learn more complex relationships than just a linear combination of inputs. <span style=\"color: rgb(181, 206, 168);\">For our first architecture, we are going to use the ReLU activation function </span>\n",
    "3. **Output layer:** final layer that produces the model's predictions. For binary classification, the output consists of 1 neuron and it often uses the <span style=\"color: rgb(181, 206, 168);\">sigmoid activation function</span> to provide the probabilities (between 0 and 1) for classifying a sample into one or two classess (1 or 0, in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A piece of advice üí°:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By including more Hidden layers in our architecture, we introduce more complexity and more learning capabilities! (Did Deep Learning come to mind? üòä). But be careful!‚ö†Ô∏è adding more hidden layers can cause the model to memorize our data and generalize poorly on the test set!.\n",
    "- You might be asking why did we choose the Relu activation activation function? Well, Machine Learning is about experimenting üß™! so are free to try others. However, there are just some considerations you need to take into account. [You can follow this link to explore in detail](https://www.geeksforgeeks.org/activation-functions-in-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last but not least, let's not forget the core mathematical concepts behind Neural Networks ... üìö "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward propagation for the neural network can be defined using the following equations:\n",
    "\n",
    "1. **Input to the Hidden Layer**:\n",
    "\n",
    "   $$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n",
    "\n",
    "   - Where:\n",
    "\n",
    "     - $W^{[1]}$  is the weight matrix of the hidden layer (composed of 5 neurons in our case)\n",
    "     - $X$ is the input feature matrix, with ***m*** number of examples (1,000 in our case) and **n** features (2 in our case)\n",
    "     - $b^{[1]}$  is the bias vector of the Hidden Layer\n",
    "     - $Z^{[1]}$  Input to the Hidden Layer (pre-activation)\n",
    "\n",
    "2. **Hidden Layer Output (Using ReLU Activation Function)**:\n",
    "\n",
    "   $$A^{[1]} = \\text{ReLU}(Z^{[1]}) \\tag{2}$$\n",
    "\n",
    "   - Where:\n",
    "\n",
    "     - $A^{[1]}$ is the activated output of the Hidden Layer\n",
    "     - $ReLU$ is the activation function applied to the output of the Hidden Layer\n",
    "\n",
    "3. **Hidden Layer to Output Layer**:\n",
    "\n",
    "   $$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n",
    "\n",
    "    - Where:\n",
    "\n",
    "    - $W^{[2]}$ is the weight matrix for the Output Layer (1 neuron in our case)\n",
    "    - $b^{[2]}$  is the bias vector for the Output Layer.\n",
    "    - $Z^{[2]}$  Hidden Layer to Output Layer (pre-activation)\n",
    "\n",
    "4. **Output Layer (Using Sigmoid Activation)**:\n",
    "\n",
    "   $$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})$$\n",
    "\n",
    "   - Where:\n",
    "\n",
    "     - $A^{[2]}$ = $\\hat{Y}$,  is the output of the network, which represents the predicted probabilities in our case, after they are passed to the sigmoid activation function.\n",
    "     - **sigmoid** is the activation function of the Output Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.2'></a>\n",
    "#### **4.2 - Setup device agonistic code**\n",
    "\n",
    "Firstly, we will need to configure our device that the model will use for training. For our learning purposes and the current environment configuration, we are only allowed to use **'cpu'**. However, just keep in mind that if we are lucky enough to have one of these powerful GPUs on our computer (or, of course, if we're using the cloud), we can really speed up the training phase!\n",
    "\n",
    "[If you are interested to know more about it, please follow this Pytorch documentation.](https://pytorch.org/get-started/locally/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.3'></a>\n",
    "#### **4.3 - Constructing the model (by subclassing nn.Module)**\n",
    "\n",
    "In this step, we will build a **simple neural network** by **subclassing** `nn.Module`, which is the foundation for all models in PyTorch. The model we‚Äôll create will have two fully connected layers (`nn.Linear`), and we‚Äôll define the **forward pass** to outline how the data flows through the network.\n",
    "\n",
    "**NOTE:** Just remember that we have initially imported our Pytorch `nn` module, when we loaded our packages: \n",
    "\n",
    "```python \n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ**Challenge 1: Model Architecture Definition**\n",
    "\n",
    "In this step, you will define the architecture of the neural network by constructing the first version of the model, **ModelV1**, using **nn.Module**. You will set up the linear layers and prepare the framework for the further steps of training and optimization. The **nn.Linear** module will be used to define Hidden Layer and Output Layer.\n",
    "\n",
    "Please consider that in this step we will only define how the layers and neurons are connected to each other. Other configurations will be performed later on the lesson.\n",
    "\n",
    "The architecture will be defined as explained on **Section 4.1 - Our First Neural Network Architecture**\n",
    "\n",
    "- The first layer should take the input features and transform them into 5 neurons\n",
    "- The second layer will receive the output from the first layer and produce 1 neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED TASK: Model Architecture Definition ###\n",
    "\n",
    "# 1. Construct our first model (version 0) that subclasses nn.Module\n",
    "class ModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        # NEURAL NETWORK ARTIFACTS #\n",
    "\n",
    "        # 2. Calling the parent class constructor to initialize the nn.Module\n",
    "        super().__init__() \n",
    "\n",
    "        # 3. Create the Linear Layers (nn.Linear) and define the hidden units #\n",
    "        # 3.1 Think about how many features come out of layer_1, and how many you want going into layer_2.\n",
    "\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        \n",
    "        #self.layer_1 = nn.Linear(in_features =   , out_features = )  \n",
    "        #self.layer_2 = nn.Linear(in_features = , out_features =  )  \n",
    "\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "\n",
    "    # 4. Define a forward() method that defines the forward pass\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward pass, data flows through the layers in a sequentially manner. \n",
    "        Here is how we apply the layers and activation functions step by step:\n",
    "\n",
    "        1. Pass data through `layer_1` (Hidden Layer 1)\n",
    "        2. Pass the result through `layer_2` (Output Layer) \n",
    "        \"\"\"\n",
    "        return self.layer_2(self.layer_1(x)) \n",
    "    \n",
    "model_v0_test(ModelV1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Neural Network Architecture:\n",
    "\n",
    "model_0 = ModelV1().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç **More to explore:** From our instantiated object , we can explore several methods that PyTorch offers and go deep dive into the configuration. \n",
    "\n",
    "For example, we can take *state_dict()*** from our **model_0** object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks familiar?** it turns out that these are the initialized weight and biases for each corresponding layer. PyTorch uses Kaiming Uniform distribution to set these values, assuring that the activations neither shrink to zero nor explode as they propagate through each layer. Although this configuration works for our purposes, [you can deep dive into more details by following this link](https://pytorch.org/docs/stable/nn.init.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_layer_1 = model_0.state_dict()['layer_1.weight'].detach().numpy()\n",
    "biases_layer_1 = model_0.state_dict()['layer_1.bias'].detach().numpy()  \n",
    "\n",
    "print('Shape of weights for the first layer',weights_layer_1.shape)\n",
    "print('Shape of biases for the first layer',biases_layer_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.4'></a>\n",
    "#### **4.4 - Defining Loss Function and Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up our model architecture, it is time to select the **loss function** and **optimizer** for training. \n",
    "\n",
    "‚úÖ The loss function helps us measure how well the model‚Äôs predictions are.\n",
    "\n",
    "‚úÖ The optimizer adjusts the model‚Äôs parameters to minimize this loss during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Choose the Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by choosing an appropriate loss function. For binary classification tasks, where the output is either 0 or 1, we commonly use **Binary Cross-Entropy Loss (BCELoss)**. However, if the model‚Äôs output is not already passed through a sigmoid activation function, you can use **BCEWithLogitsLoss**. This loss function combines the sigmoid activation and the binary cross-entropy loss in one step, making it computationally more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rember that our Loss Function for the Binary Classifier Architecture Network is represented as:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\sigma(z_i)) + (1 - y_i) \\log(1 - \\sigma(z_i)) \\right] \n",
    "$$\n",
    "\n",
    "- Where:\n",
    "\n",
    "    - $y_{i}$ is the true label for the i\\-th sample in the batch, and it can be `0` or `1`\n",
    "    - $z_{i}$  is the logit output from the model (before applying the sigmoid function)\n",
    "    - N is the total number of samples in the current batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that logits are the raw scores from the model (often called log-odds), and they are produced by the final layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ**Challenge 2: Loss Function Definition**\n",
    "\n",
    "Use **`BCEWithLogitsLoss()`** from the **`torch.nn` module** to create the **`loss_fn`** for the binary classification task. Do **not** include any additional parameters in the initialization. Ensure the loss function is correctly initialized and ready to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED TASK: Loss Definition ###\n",
    "\n",
    "### YOUR CODE STARTS HERE ###\n",
    "# loss_fn = \n",
    "#### YOUR CODE ENDS HERE ###\n",
    "grade_loss_fn(loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Setting up the Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will select an optimizer. The optimizer's goal is to adjust the model's parameters (weights and biases) during training. The most basic optimizer is **Stochastic Gradient Descent (SGD)**, which updates the parameters based on the gradients of the loss function with respect to the parameters. As we have seen in previous lessons, the SGD helps to overcome slow convergence and improves optimization speed. But as stated before,  Machine Learning is about experimenting üß™! So you are welcome to try the ones we have seen in previous lessons, and also following the next link, to see [the optimizers available in Pytorch](https://pytorch.org/docs/stable/optim.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formula:\n",
    "\n",
    "The update rule for Stochastic Gradient Descent is:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\cdot \\nabla_\\theta J(\\theta) \\tag{1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ is the vector of the model's parameters (weights and biases)\n",
    "- $\\nabla_\\theta J(\\theta)$ is the gradient of the loss function $J(\\theta)$ with respect to the model parameters $(\\theta)$.\n",
    "- $\\alpha$ is the **learning rate** that controls how much the parameters are updated in each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How it works?\n",
    "\n",
    "‚úÖ  **The Gradient Computation**: The gradient of the loss function is computed with respect to the model parameters. For SGD, the gradient is computed using **one sample** (But it can be configured to take small batches) at a time.\n",
    "\n",
    "‚úÖ  **The Parameter Update**: The model parameters $\\theta$ are updated by subtracting a fraction of the calculated gradient and scaled by the learning rate $(\\alpha)$.\n",
    "\n",
    "‚úÖ  **This is an iterative Process**: This process is repeated for each sample (or mini-batch) in the dataset, in order to gradually improving the model parameters to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö° \"Don't worry, everything will start to make sense as we dive into the next challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ**Challenge 3: Optimizer Configuration**\n",
    "\n",
    "Your goal for this challenge is to complete the configuration of our optimizer! The Python class **OptimizerConfigurator** has already been set up for you to instantiate the optimizer object. Your task is to correctly set the learning rate to **10%** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED FUNCTION: Optimizer Configuration\n",
    "\n",
    "class OptimizerConfigurator:\n",
    "    \"\"\"\n",
    "    A class to configure the SGD optimizer for a given model with a specified learning rate.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The model to optimize (PyTorch model).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = None      \n",
    "    def configure_optimizer(self):\n",
    "        \"\"\"\n",
    "        Configures the SGD optimizer with the given model and learning rate.\n",
    "        \n",
    "        Returns:\n",
    "        - optimizer: The configured optimizer.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        # self.optimizer = torch.optim.SGD(params=self.model.parameters(), lr =)\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "        return self.optimizer\n",
    "\n",
    "optimizer_configurator = OptimizerConfigurator(model_0)\n",
    "optimizer = optimizer_configurator.configure_optimizer()\n",
    "test_optimizer_configuration(optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.5'></a>\n",
    "#### **4.5 - Building a Training and Testing Loop** üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are reaching out our final step in developing our Neural Network for binary classification! So things are going to turn very interesting from now on... ready? üî•\n",
    "\n",
    "In this section, you are going to implement üõ†Ô∏è the training and evaluation loop üîÑ for a neural network model using PyTorch. This process involves training the model, updating its parameters, and then evaluating its performance on the test data after each iteration (i.e. **epoch**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But before moving on, let's not forget the core functionality of **BACKPROPAGATION**: üìö\n",
    "\n",
    "‚ú® Backpropagation is the magic behind how a neural network learns from data!\n",
    "\n",
    "‚úÖ It uses the Loss function to assess how the model performs with the actual data. After the loss is computed, it applies the **chain rule** to calculate the gradients (how much each parameter contributes to the loss). \n",
    "\n",
    "‚úÖ We use the **chain rule** to propagate the error backwards from the output layer to the input layer\n",
    "\n",
    "For each layer, the gradient is computed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial Z^{[l]}} \\cdot \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}}\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "    \n",
    "    - $\\frac{\\partial L}{\\partial W^{[l]}}$ is the gradient calculated using the chain rule\n",
    "\n",
    "‚úÖ Essentially, we somewhat **trace** the error back through the network to compute how each weight and bias in the network contributed to the final error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéØ**Challenge 4: Training and Evaluation Loop**\n",
    "\n",
    "In this task, you will implement a training loop for your model. You will iterate through multiple epochs, performing the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Set reproducibility**: set random seed to ensure that the results are reproducible \n",
    "\n",
    "2. **Setting No. of epochs**: how many times the model **will see** the entire training data?\n",
    "\n",
    "3. **Training Loop** üîÑ: for each iteration (epoch) ...\n",
    "\n",
    "    3.1 **Forward Pass**: We set the model into **training mode**. We also compute the logits and apply the sigmoid function. \n",
    "\n",
    "    3.2 **Calculate Loss and Accuracy**: Compute the loss function and accuracy for each iteration.\n",
    "\n",
    "    3.3 **Zeroing the Gradients**: Before performing backpropagation, we use the **zero_grad()** (zero the gradients) to the <span style=\"color: rgb(181, 206, 168);\">optimizer object</span>. PyTorch accumulates gradients by default, so we need to clear them out before computing the gradients for the current batch.\n",
    "\n",
    "    3.4 **Backpropagation**: Use the **backward() method** to compute the gradient of the <span style=\"color: rgb(181, 206, 168);\">loss object</span> with respect to the model parameters.\n",
    "\n",
    "    3.5 **Optimizer Step (Gradient Descent)**: It uses **step() method** to apply the gradient updates to the model parameters using the <span style=\"color: rgb(181, 206, 168);\">optimizer object </span>, during backpropagation. \n",
    "\n",
    "4. **Evaluation Step** üìã: After the training step, we evaluate how well the model performs on the test data.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already prepared the core artifacts of the process, but you will need to complement the training and evaluation loop by filling in the missing pieces üß©. We have imported the **accuracy_fn** which takes the **true labels** and **predicted labels**, respectively, to estimate the accuracy properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setting reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 2. Setting No. of Epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Put data on the target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED TASK: 3.Training and Evaluation Loop ###\n",
    "\n",
    "class TrainingEvaluationLoop:\n",
    "    \"\"\"\n",
    "    The class to configure and execute the training evaluation loop\n",
    "    \n",
    "    Parameters:\n",
    "    model: The model to optimize (PyTorch model)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, epochs, optimizer, loss):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "\n",
    "    def execute_training_loop(self):\n",
    "        \"\"\"\n",
    "        Performs the training loop for each epoch\n",
    "\n",
    "        Returns:\n",
    "        - optimizer: The configured optimizer.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            # 3.1 Forward Pass  \n",
    "            self.model.train()\n",
    "            y_logits = self.model(X_train).squeeze()\n",
    "            y_pred = torch.round(torch.sigmoid(y_logits))  \n",
    "            # 3.2 Calculate the Loss and Accuracy                \n",
    "            loss = self.loss(y_logits, y_train)\n",
    "            acc = accuracy_fn(y_true = y_train,\n",
    "                                y_pred = y_pred)\n",
    "               \n",
    "            # 3.3 Use the self.optimizer to set zeroing the gradients #\n",
    "            #### YOUR CODE STARTS HERE ###\n",
    "            #\n",
    "            #### YOUR ENDS ENDS HERE ###\n",
    "    \n",
    "            for param in self.model.parameters():\n",
    "                # Before backward pass, gradients should be None (zeroed out)\n",
    "                 assert param.grad is None, f\"Error: You have not zeroed the gradients! Please check challenge 4 instructions!\"\n",
    "              \n",
    "            # 3.4 Use the loss object to set the backward backpropagation \n",
    "            ### YOUR CODE STARTS HERE ###\n",
    "            # \n",
    "            ### YOUR CODE ENDS HERE ###\n",
    "              \n",
    "            for param in self.model.parameters():\n",
    "                 assert param.grad is not None, f\"Error: Loss Backpropagation not properly configured Please check challenge 4 instructions!\"\n",
    "           \n",
    "            # 3.5 Setting Optimizer step for updating the gradients\n",
    "\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # 4. Evaluation Step\n",
    "            self.model.eval()\n",
    "            with torch.inference_mode():\n",
    "                test_logits = self.model(X_test).squeeze()\n",
    "                test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "                test_loss = self.loss(test_logits,\n",
    "                                    y_test)\n",
    "                test_acc = accuracy_fn(y_true = y_test,\n",
    "                                        y_pred = test_pred)\n",
    "                \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}%, | Test Loss:  {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n",
    "        \n",
    "        print('\\033[92mAll tests passed!')\n",
    "        return self.model     \n",
    "\n",
    "model_0 = ModelV1().to(device)\n",
    "optimizer_configurator = OptimizerConfigurator(model_0)\n",
    "optimizer = optimizer_configurator.configure_optimizer()\n",
    "train_val_loop_config = TrainingEvaluationLoop(model_0, epochs, optimizer, loss_fn)\n",
    "model_artifact = train_val_loop_config.execute_training_loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deep dive into the results! let's plot our decision boundary from our classifier! üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_artifact, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_artifact, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm ... Something is Missing! ü§î\n",
    "\n",
    "As you can see we have successfully built a classifier, but... it‚Äôs not performing as well as we expected. and we are missing something essential. Can you guess what might be causing this? (take a couple of minutes to think through) üßê"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it looks like we've accidentally built a classifier using a linear regression approach! üò¨\n",
    "\n",
    "Why did that happen? Because we forgot to add the **non-linearity**! In simpler terms, we didn‚Äôt apply any activation functions on the hidden layer. But don‚Äôt worry, we‚Äôve got you covered! This was totally intentional ‚Äî we just wanted to show you the true power behind neural networks. üß†\n",
    "\n",
    "Now, it's your turn! You‚Äôre challenged to bring the power of non-linearity to our model. Oh, and remember when we hinted that you might be tempted to add more layers? üòè"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move forward, we want to point out that there are many ways you can boost the performance of Neural Networks. Here are a few ideas to get you started:\n",
    "\n",
    "- Adding more Hidden Layers\n",
    "- Increasing the number of neurons in each layer\n",
    "- Increasing the number of epochs\n",
    "- Trying out different activation functions\n",
    "- Experimenting with various optimizers\n",
    "\n",
    "... and this is just the beginning! Feel free to explore and experiment further using this notebook, for your personal projects. Once you tackle the next challenge, you'll have the skills to start experimenting on your own. \n",
    "\n",
    "And yet again ...\n",
    "\n",
    "**Remember:** Machine Learning is all about learning through experimentation! üß™üß™üß™ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ**Challenge 4: Adding Non-Linearity + More Hidden Layers!**\n",
    "\n",
    "At this point you have mastery the configuration of Neural Networks using Pytorch. Great job so far! \n",
    "\n",
    "1. Now, it's time to enhance the **original architecture** by adding 3 more layers. Each layer should be composed of:\n",
    "\n",
    "- The first layer should take 2 input features and set <span style=\"color: rgb(181, 206, 168);\">10 neurons</span> to this configuration. \n",
    "- The second layer will take the number of features from the first layer, and set <span style=\"color: rgb(181, 206, 168);\">10 neurons</span> to this configuration.\n",
    "- The third layer will take the output of the second layer and output <span style=\"color: rgb(181, 206, 168);\">1 neuron</span>.\n",
    "\n",
    "2. Also, you should not forget to include our activation function for each layer by using the Pytorch **nn** Module:\n",
    "\n",
    "- nn.ReLU()\n",
    "\n",
    "\n",
    "To help you out, you don't need to worry (but not limited to continue experimenting on your own) about the optimizer and loss_fn configuration. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED TASK: Adding Non-Linearity + Hidden Layers ###\n",
    "\n",
    "class CircleModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # NEURAL NETWORK ARTIFACTS #\n",
    "        # 1. ADDING MORE LAYERS TO INCREASE DEPTH\n",
    "        # Think about how many features come out from the current layer, and how many you want going into the next one.\n",
    "        self.layer_1 = nn.Linear(in_features = 2, out_features = 10)  \n",
    "\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        #self.layer_2 = # Second Hidden Layer\n",
    "        #self.layer_3 = # Final output layer\n",
    "        ### YOUR CODE ENDS HERE ### \n",
    "\n",
    "        # 2. ADDING ACTIVATION FUNCTION FUNCTIONALITY\n",
    "        # We will use an activation function after each hidden layer to introduce non-linearity\n",
    "        \n",
    "        ###YOUR CODE STARTS HERE###\n",
    "        #self.relu = \n",
    "        ###YOUR CODE ENDS HERE###\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward pass, data flows through the layers sequentially. \n",
    "        Here's how we apply the layers and activation functions step by step:\n",
    "\n",
    "        1. Pass data through `layer_1` (Hidden Layer 1) and define the activation function for its output\n",
    "        2. Pass the result through `layer_2` (Hidden Layer 2) and define the activation function for its output\n",
    "        3. Pass the result through `layer_3` (Output Layer) (no activation after it because it's the final prediction)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "\n",
    "        #x = self.relu()    # First Layer Definition     \n",
    "        #x = self.relu()    # Second Layer Definition\n",
    "        #x = self.layer_3() # Output Layer Definition\n",
    "\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "\n",
    "        return x\n",
    "\n",
    "model_2 = CircleModelV2().to(device)\n",
    "optimizer_configurator_v2 = OptimizerConfigurator(model_2)\n",
    "optimizer_v2 = optimizer_configurator_v2.configure_optimizer()\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_v2(CircleModelV2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loop_config = TrainingEvaluationLoop(model_2, epochs, optimizer_v2, loss_fn)\n",
    "model_artifact_v2 = train_val_loop_config.execute_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_artifact_v2, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_artifact_v2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a look at the decision boundary!!!**\n",
    "\n",
    "Now we‚Äôre back to business! We've built our binary classifier using neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÜCongratulations! You've Made It! üèÜ \n",
    "\n",
    "Well done! You've finally done it! You've successfully built and **binary classifier neural network**!. Surely, it was a lot to digest, but you have done an incredible progress. Let's quickly recap the key concepts you've learned throught this lesson:\n",
    "\n",
    "1. **Configuring Neural Networks:** You‚Äôve learned how to set up a neural network with hidden layers, apply activation functions, and define the forward pass.\n",
    "2. **Understanding Non-Linearity:** You discovered how important activation functions (like ReLU) are for introducing non-linearity and improving a model performance.\n",
    "3. **Implementing the Training Loop**: You've learned how to configure and implement a training loop with backpropagation and gradient descent.\n",
    "4. **Loss Function & Optimizer Setup:** You set up loss functions like **BCEWithLogitsLoss** and configured optimizers like SGD, getting familiar with how they guide model training.\n",
    "5. **Experimentation:** You have also learned that Machine Learning is all about experimenting!\n",
    "\n",
    "You are now officially a professional in the field of neural networks! üåü \n",
    "\n",
    "The foundation is laid, and now it‚Äôs time to experiment. Apply what you‚Äôve learned and try using your own data to see how the model performs. Continue refining, adjusting, and exploring new possibilities.\n",
    "\n",
    "Remember, Machine Learning is a journey, and every experiment gets you closer to mastery. Keep learning, keep testing, and who knows? You might be the next to revolutionize the world with your innovations! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
